---
title: "Assignment 1: Analyzing an Obesity dataset"
author: William Schaafsma, Hau Nguyen, Qingyu Meng
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
       highlight: textmate
       theme: flatly
       number_sections: TRUE
       toc: TRUE
       toc_float:
         collapsed: TRUE
         smooth_scroll: FALSE
---

# Introduction And Research Question

In this assignment an obesity dataset, retrieved from [Palechor & Manotas, 2019](https://www-sciencedirect-com.proxy.library.uu.nl/science/article/pii/S2352340919306985?via%3Dihub) will be analyzed. The data gathered for this paper comes from individuals that live in Peru, Mexico and Colombia. It's stated that 23% of the data is "real" and that 77% has been synthetically generated.

The goal for this assignment is to build a model that most accurately predicts the BMI of *new* patients. Understanding which predictors have great influence for one's BMI is favorable for policymakers regarding national health. Moreover, patients get more understanding of their own health.

We will work in an exploratory manner. Thus, in this assignment we are particularly interested in: 'What predictors show the greatest influence regarding a person's BMI?'

# Packages

The following libraries are required to run this program succesfully.
```{r libraries, warning = FALSE, message = FALSE}
library(ggplot2)
library(corrplot)
library(dplyr)
library(knitr)
library(tidyverse)
library(mice)
library(magrittr)
library(readr)
library(caret)
library(leaps)
library(glmnet)
```


# Loading The Dataset & Set Seed

First we load the data and set the seed in order to generate a reproducible result.

```{r, message=FALSE}
# loading data
obese <- read_csv("ObesityDataSet_raw_and_data_sinthetic (2)/ObesityDataSet_raw_and_data_sinthetic.csv")

# set the seed
set.seed(1705)
```

# Processing Of The Data

We want to include BMI as a dependent variable in our model. Thus we have to derive BMI from Weight and Height with the formula:  $BMI = Weight / Height^2$

Since we use BMI as the dependent variable, high multicollinearity problems will evolve in *Height* and *Weight* for example in building a model or correlations. Therefore we will delete these predictors since they've become irrelevant.

```{r, message=FALSE, }
# calculating bmi from height and weight
Bmi <- data.frame(Bmi = (obese$Weight / (obese$Height^2)))

# adding bmi to the original dataframe
obese_mid <- cbind(obese, Bmi)

# removing *NObeyesdad* since we don't need the variable
obese_complete <- select(obese_mid, -c(NObeyesdad, Height, Weight))
```

# Splitting The Data Into Train, Validation & Test

In order to evaluate the performance of the model we will build later on, we have to split the data into three partitions: Train, Test, Validation. The data will be split in a way that ~70% of the data is used for training the model, ~20% for validating the model and ~10% for testing the model.

```{r, message=FALSE}
# partition of training
part_train <- createDataPartition(obese_complete$Bmi, p = .7, 
                                  list = FALSE, 
                                  times = 1)

# creation of training data
obese_train <- obese_complete[part_train,]

# remainder for validation and testing
part_test_val <- obese_complete[-part_train,]

# partition of validation
part_val <- createDataPartition(part_test_val$Bmi, p = .66, 
                                  list = FALSE, 
                                  times = 1)
# creation of validation data
obese_val <- part_test_val[part_val,]

# creation of test data
obese_test  <- part_test_val[-part_val,]
```

# Explaining The Variables

**WARNING:** the simulated data created incorrect data. Particularly, it treated categorical data as numeric data and therefore wrongfully imputed these data. However, in this assignment we will treat these data as if they were numeric since converting the datatypes causes loads of parsing errors. This problem affects the following variables: *FCVC, NCP, CH2O, FAF* and *TUE*.


In this section, an elaboration on the variables is given. For starters, the categorical variable **Gender** consists of two categories representing one's gender: male or female.

Next is the continuous variable **Age** that only contains integers of every person's age. 

Then the continuous variable **Height** that contains floating values for a person's Height, measured in meters.

Then there's another continuous variable **Weight** that also contains floating values for a person's weight, measured in Kilograms (KG).

There's also the dichotomous variable **family_history_with_overweight** which controls for possible genetic predisposition for a high BMI. 

Next is another dichotomous variable **FAVC** that represents whether a person frequently consumes high caloric foods.

Then a "numeric" variable **FCVC** which represents the frequency of consuming vegetables. 

Then there's another "numeric" variable **NCP** that accounts for the number of main meals each day.

Next is the categorical variable **CAEC** consisting of four categories representing the consumption of foods between meals: No, Sometimes, Frequently, Always.

Then there is a dichotomous variable **SMOKE** that represents whether a person smokes

Next is a "numeric" variable **CH2O** which represents the amount of water a person drinks each day.

Next is another dichotomous variable **SCC** that accounts for calorie checking.

Then there's a "numeric" variable **FAF** which represents the frequency of physical activity of a person.

Next is another "numeric" variable **TUE** which represents the amount a person spends on it's devices such as a phone.

Then there is the categorical variable **CALC** consisting of four categories representing a person's alcohol intake: Nothing, Sometimes, Frequently, Always. 

Finally, there is the categorical variable **MTRANS** consisting of five categories representing a person's most used public transport: Automobile, Motorbike, Bike, Public Transportation and Walking.


# Getting To Know The Data

In order to work with a dataset, we need to understand the dataset. First, let's check wether there are missing data in our dataset. To do this we use the `mice` package.

```{r, message=FALSE}
md.pattern(obese_complete)
```

Fortunately, we can conclude that our dataset contains no missing values. Furthermore, the figure shows that our dataset holds 17 variables and 2111 observations. 

Now, let us see the head and the summary of the dataset

```{r head}
head(obese_complete) %>% 
  knitr::kable(format = "markdown", digits= 1, padding = 30, align = 'c')
```

```{r}
summary(obese_complete)
```
From the head and summary function we can tell that the data has loaded as expected and that the datatypes are in line with how we described them in section **"Explaining the variables"**. 

# Correlations

In this section we will try to look for strong correlations between the predicting variables and our dependent variable BMI. 

**NEEDS FULL WORK, E.G. VISUALIZATIONS OF CORRELATIONS**


# Model Training

Building a model by trial and error takes a lot of time and working memory. It's best to build a model based on some algorithm. First let's prove this first statement. The following piece of code will calculate the number of possible models given the fact that we would like a model with at least 4 predicting variables.

```{r}
# load a necessary source
source("generate_formulas.R")
```

```{r}
# create a vector for all predicting variables except the dependent variable bmi

x_vars <- colnames(obese_complete)
x_vars <- x_vars[x_vars != "Bmi"]

# calculate the number of possible models

n_possible_models <- generate_formulas(p=4, x_vars = x_vars, y_var = "Bmi") 
length(n_possible_models)
```

Given the assumption for a model containing 4 predicting variables, there are 1001 possible models. Thus, let's look at algorithms that calculate the "best model".

## Forward Stepwise Selection

For example we can use *forward selection*. This method start with no predictors and then iteratively adds the most contributive predictors until there's no significant improvement. 
**MORE EXPLANATION ABOUT FORWARD SELECTION**


```{r}
# start building a model by forward selection
model_forward <- regsubsets(Bmi ~., data = obese_train, 
                                 method = "forward")

summary(model_forward)
```
## Backward Stepwise Selection

Then there is also *backward selection* this method is the complete opposite of *forward selection*. Here the algorithm starts with the complete model and then iteratively starts eliminating the least significant variables until all significant variables remain.
**MORE EXPLANATION ABOUT BACKWARD SELECTION**

```{r}
model_backward <- regsubsets(Bmi ~., data = obese_train, 
                                 method = "backward")

summary(model_forward)
```
## LASSO Regression; step-by-step

The final method provided in this assignment is the LASSO regression method. The Least Absolute Shrinkage and Selection Operator is a powerful method since it can handle very large data sets. Furthermore this method avoids overfitting.
**MORE EXPLANATION ABOUT LASSO**

```{r}
# set up basics for lasso regression
pred_variables <- model.matrix(Bmi~., bind_rows(obese_train, obese_val))[, -1]

# find the best lambda for lasso

cv <- cv.glmnet(x = pred_variables, y = c(obese_train$Bmi, obese_val$Bmi), nfolds = 15)

best_lambda <- cv$lambda.min


# train the lasso model

model_lasso <- glmnet(pred_variables, c(obese_train$Bmi, obese_val$Bmi),
                      alpha = 1, lambda = best_lambda)

coef(model_lasso)

```


## Dummy Coding
**needs explanation**

```{r}
obese_train$CAEC <- ifelse(obese_train$CAEC == 'Frequently')
```

# Model Evaluation & VIF

# Checking Assumptions

# Visualizations

# Answering Research Question

# Discussion & Limitations

# Sources
