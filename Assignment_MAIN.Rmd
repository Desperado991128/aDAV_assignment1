---
title: "Assignment 1: Analyzing an Obesity dataset"
author: William Schaafsma, Hau Nguyen, Qingyu Meng
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
       highlight: textmate
       theme: flatly
       number_sections: TRUE
       toc: TRUE
       toc_float:
         collapsed: TRUE
         smooth_scroll: FALSE
---

# Introduction And Research Question

In this assignment an obesity dataset, retrieved from [Palechor & Manotas, 2019](https://www-sciencedirect-com.proxy.library.uu.nl/science/article/pii/S2352340919306985?via%3Dihub) will be analyzed. The data gathered for this paper comes from individuals that live in Peru, Mexico and Colombia. It's stated that 23% of the data is "real" and that 77% has been synthetically generated.

The goal for this assignment is to build a linear model that most accurately predicts the BMI of *new* patients, therefore we mainly focus on prediction. However, from the inference perspective, understanding which predictors have great influence for one's BMI is favorable for policymakers regarding national health. Moreover, patients will get more understanding of their own health.

We will work in an exploratory manner. Thus, in this assignment we are particularly interested in: *'What predictors show the greatest influence regarding a person's BMI?'*

# Loading packages

The following libraries are required to run this program successfully.

```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(corrplot)
library(MASS)
library(dplyr)
library(knitr)
library(tidyverse)
library(mice)
library(magrittr)
library(readr)
library(caret)
library(leaps)
library(glmnet)
library(psych)
library(fastDummies)
library(stats)
library(vcd)
library(graphics)
library(olsrr)
```


# Loading The Dataset & Set Seed

First we load the data and set the seed in order to generate a reproducible result.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# loading data
obese <- read_csv("ObesityDataSet_raw_and_data_sinthetic (2)/ObesityDataSet_raw_and_data_sinthetic.csv")

# set the seed
set.seed(2022)
```

# Processing Of The Data

We want to include BMI as a dependent/response variable in our model, but we only have the 'Weight' and 'Height' columns in hand. To do this we need to derive BMI from Weight and Height with the formula:  $BMI = Weight / Height^2$

Since we use BMI as the dependent variable, high multicollinearity problems will evolve in *Height* and *Weight* for example in building a model or evaluating their correlations. Therefore we will delete these predictors since they've become irrelevant after the introduction of the BMI variable. Besides, the 'NObeyesdad' column concerns body weight grouping, since we will not involve in a classification task in this project, this column is never in use and we can directly delete it.

```{r, message=FALSE, }
# calculating BMI from height and weight
Bmi <- data.frame(Bmi = (obese$Weight / (obese$Height^2)))

# adding BMI to the original dataframe
obese_mid <- cbind(obese, Bmi)

# removing *NObeyesdad* since we don't need the variable
obese_complete <- subset(obese_mid, select = -c(NObeyesdad, Height, Weight))
```


Our dataset contains several categorical variables ("Gender", "family_history_with_overweight", "FAVC", "SMOKE", "SCC", "CALC", "CAEC", "MTRANS"), which needs to be dummy-coded later times in order to gain a meaningful interpretation after building the models. 

First, we convert the categorical columns into factors, which includes "Gender", "family_history_with_overweight", "FAVC", "SMOKE", "SCC", "CALC", "CAEC", "MTRANS". Note that some of them are categorical variables but in numeric type.

```{r}
# converting mentioned columns into factor variables
obese_complete$Gender <- as.factor(obese_complete$Gender)

obese_complete$family_history_with_overweight <- as.factor(obese_complete$family_history_with_overweight)

obese_complete$FAVC <- as.factor(obese_complete$FAVC)
obese_complete$SMOKE<- as.factor(obese_complete$SMOKE)
obese_complete$SCC <- as.factor(obese_complete$SCC)
obese_complete$CALC <- as.factor(obese_complete$CALC)
obese_complete$CAEC <- as.factor(obese_complete$CAEC)
obese_complete$MTRANS <- as.factor(obese_complete$MTRANS)
```

To aid our interpretation for the dummy coefficients, we reassign the reference level for some columns.

```{r}
obese_complete$CALC <- relevel(obese_complete$CALC, ref = "no")
obese_complete$CAEC <- relevel(obese_complete$CAEC, ref = "no")
obese_complete$MTRANS <- relevel(obese_complete$MTRANS, ref = "Walking")
```



# Splitting The Data Into Train, Validation & Test

In order to evaluate the performance of the model we will build later on, we have to split the data into three partitions: Train, Test, Validation. The data will be split in a way that ~70% of the data is used for training the model, ~20% for validating the model and ~10% for testing the model.

```{r, message=FALSE}
# partition of training
part_train <- createDataPartition(obese_complete$Bmi, p = .7, 
                                  list = FALSE, 
                                  times = 1)

# creation of training data
obese_train <- obese_complete[part_train, ]

# remainder for validation and testing
part_test_val <- obese_complete[-part_train, ]

# partition of validation
part_val <- createDataPartition(part_test_val$Bmi, p = .66, 
                                list = FALSE, 
                                times = 1)
# creation of validation data
obese_val <- part_test_val[part_val, ]

# creation of test data
obese_test  <- part_test_val[-part_val, ]
```

To verify our dataset was normally distributed, we can make a corresponding histogram and density plot.

```{r}
# Histogram of the Bmi distribution in spliting datasets
ggplot() + 
  geom_histogram(data = obese_train, mapping = aes(x = Bmi), alpha = 0.45, colour = "Purple", binwidth = 5) + 
  geom_histogram(data = obese_val, mapping = aes(x = Bmi), alpha = 0.45, colour = "Yellow", binwidth = 5) + 
  geom_histogram(data = obese_test, mapping = aes(x = Bmi), alpha = 0.45, colour = "Brown", binwidth = 5)  
```


```{r}
# Histogram of the Bmi distribution in spliting datasets
ggplot() + 
  geom_density(data = obese_train, mapping = aes(x = Bmi), alpha = 0.45, colour = "Purple") + 
  geom_density(data = obese_val, mapping = aes(x = Bmi), alpha = 0.45, colour = "Yellow") + 
  geom_density(data = obese_test, mapping = aes(x = Bmi), alpha = 0.45, colour = "Brown")  
```

Each of the plots above are highly overlapping, suggesting that the obese_complete dataset was randomly and normally distributed into the training set, validation set and test set respectively.

# Explaining The Variables

**WARNING:** the simulated data created incorrect data. Particularly, it treated categorical data as numeric data and therefore wrongfully imputed these data. However, in this assignment we will treat these data as if they were numeric since converting the datatypes causes loads of parsing errors. This problem affects the following variables: *FCVC, NCP, CH2O, FAF* and *TUE*.


In this section, an elaboration on the variables is given. For starters, the categorical variable **Gender** consists of two categories representing one's gender: male or female.
Next is the continuous variable **Age** that only contains integers of every person's age. 

Then the continuous variable **Height** that contains floating values for a person's Height, measured in meters.
Then there's another continuous variable **Weight** that also contains floating values for a person's weight, measured in Kilograms (KG).

There's also the dichotomous variable **family_history_with_overweight** which controls for possible genetic predisposition for a high BMI. 
Next is another dichotomous variable **FAVC** that represents whether a person frequently consumes high caloric foods.

Then a "numeric" variable **FCVC** which represents the frequency of consuming vegetables. 
Then there's another "numeric" variable **NCP** that accounts for the number of main meals each day.

Next is the categorical variable **CAEC** consisting of four categories representing the consumption of foods between meals: No, Sometimes, Frequently, Always.
Then there is a dichotomous variable **SMOKE** that represents whether a person smokes

Next is a "numeric" variable **CH2O** which represents the amount of water a person drinks each day.
Next is another dichotomous variable **SCC** that accounts for calorie checking.

Then there's a "numeric" variable **FAF** which represents the frequency of physical activity of a person.
Next is another "numeric" variable **TUE** which represents the amount a person spends on it's devices such as a phone.

Then there is the categorical variable **CALC** consisting of four categories representing a person's alcohol intake: Nothing, Sometimes, Frequently, Always. 
Finally, there is the categorical variable **MTRANS** consisting of five categories representing a person's most used public transport: Automobile, Motorbike, Bike, Public Transportation and Walking.


# Getting To Know The Data

In order to work with a dataset, we need to understand the dataset. First, let's check wether there are missing data in our dataset. To do this we use the `mice` package.

```{r, message=FALSE, comment=NA}
md.pattern(obese_complete, rotate.names = TRUE)
```

Fortunately, we can conclude that our dataset contains no missing values. Furthermore, the figure shows that our dataset holds 15 variables and 2111 observations. 

Now, let us see the head of the data.

```{r head}
head(obese_complete) %>% 
  knitr::kable(format = "markdown", digits= 1, padding = 30, align = 'c')
```

Let's also check the describe function.
```{r, comment=NA}
describe(obese_complete)
```
From the head and describe function we can tell that the data has loaded as expected and that the datatypes are in line with how we described them in section **"Explaining the variables"**. 


# Exploratory Data Analysis

Suppose we want to look for a difference in the average Bmi of a person based on the CAEC variable. We are thus looking for a relationship between a continuous and (nominal/ordinal) categorical variable. Therefore, we should make a boxplot to look for this relationship graphically. Same ideas applies for other pairs of such relationships, in which we should also make boxplots to look into the data. Considering the limitation of lengths for this section, we will only explore the following variables pairs: CAEC ~ Bmi, Gender ~ Bmi and MTRANS ~ Bmi.

```{r}
# Visualizations of individual variables -- CAEC
ggplot(obese_complete, aes(x = CAEC, y = Bmi, color = CAEC)) +
  geom_boxplot() +
  labs(title = "Values of Bmi for each level of consumption of foods between meals")
```


```{r}
# 95% confidence intervals for the CAEC mean
ci_CAEC <- \(x) mean_se(x, mult = 1.96)
ggplot(obese_complete, aes(CAEC, Bmi, color = CAEC)) +
  stat_summary(fun.data = ci_CAEC) +
  labs(title = "95% confidence intervals for the CAEC mean")
```

Next, we draw the graph for the case when "Gender" is the explanatory variable.

```{r}
# Visualizations of individual variables -- Gender
ggplot(obese_complete, aes(x = Gender, y = Bmi, color = Gender)) +
  geom_boxplot() +
  labs(title = "Values of Bmi for each Gender")
```

Based on the boxplot above we can see that the average Bmi of Males is slightly greater than the average Bmi of Females. However, we cannot exclude the possibility that the effects we see could be generated random variations. To verify this, we need to check how many samples we have in each of the categories in the "Gender" variable.

```{r, comment=NA}
table(obese_complete$Gender)
```
Bingo! It turns out our "Gender" column has a perfectly healthy distributions, with its each level exactly having the same number of samples (1043 - 1068).


```{r}
# 95% confidence intervals for the Gender mean
ci_gender <- \(x) mean_se(x, mult = 1.96)
ggplot(obese_complete, aes(Gender, Bmi, color = Gender)) +
  stat_summary(fun.data = ci_gender) +
  labs(title = "95% confidence intervals for the Gender mean")
```

Then, let's look at how does the different level for "MTRANS" variables affects average Bmi.

```{r}
# Visualizations of individual variables -- MTRANS
ggplot(obese_complete, aes(x = MTRANS, y = Bmi, color = MTRANS)) +
  geom_boxplot() +
  labs(title = "Values of Bmi for each transportation type")
```

```{r}
# 95% confidence intervals for the MTRANS mean
ci_mtrans <- \(x) mean_se(x, mult = 1.96)
ggplot(obese_complete, aes(MTRANS, Bmi, color = MTRANS)) +
  stat_summary(fun.data = ci_mtrans) +
  labs(title = "95% confidence intervals for the transportation mean")
```

As did for the "Gender ~ Bmi" pair, here we check the distribution of the column "MTRANS" again.

```{r, comment=NA}
table(obese_complete$MTRANS)
```
From the table we can say the shape for the "MTRANS" variable is highly biased, with majority of data being the type "Public_Transportation", and very few data with the type "Bike".


# Correlations

In this section we will try to look for strong correlations between the predicting variables and our dependent variable BMI. The aim is to see how the variables seem to interact with each other, as well as to find the correlations between them.

To show the overall correlations in our dataset, we use featurePlot() function from the caret package to display a scatterplot matrix.

**NB: as you can see below, the plot is very messy, needs adjusted. In case this is not doable, we should consider cross tabs (the table function) instead, or Cramer's V. Do you have any suggestions?** 

```{r}
continuous_variables <- select(obese_complete, -c("Gender", "family_history_with_overweight", "FAVC", "SMOKE", "SCC", "CALC", "CAEC", "MTRANS"))
categorical_variables <- select(obese_complete, c("Gender", "family_history_with_overweight", "FAVC", "SMOKE", "SCC", "CALC", "CAEC", "MTRANS"))

featurePlot(x = continuous_variables, y = categorical_variables, plot = "pairs")
```

We know that if all concerned variables are continuous numerical, we can use Pearson Correlation Coefficient to compute correlations between each pair of them. However, since in our dataset the categorical variables are all nominal (or partially ordinal), we should instead consider another correlation metrics: Cramerâ€™s V. This is used to calculate the correlation between nominal categorical variables.


```{r, comment=NA}
cat_vars = c("Gender", "family_history_with_overweight", "FAVC", "SMOKE", "SCC", "CALC", "CAEC", "MTRANS")
cat_df = data.frame(categorical_variables)

catcorrm <- function(vars, dat) sapply(vars, function(y) sapply(vars, function(x) assocstats(table(dat[,x], dat[,y]))$cramer))
catcorrm(cat_vars, cat_df)
```

The result produced by catcorrm() function is a correlation matrix of Cramer's V's. From this matrix we can see several pairs of categorical variables has relatively high correlations, such as 'Gender ~ MTRANS', 'family_history_with_overweight ~ FAVC/SCC/CAEC', 'FAVC ~ SCC', 'FAVC ~ MTRANS', 'SCC ~ CAEC' and 'CAEC ~ FAVC'.

For numeric variables in the dataset, we can use corrplot() function to visualize their correlations.

```{r}
corrplot(cor(continuous_variables), 
         method = "circle",
         type = "full",
         tl.pos = "tl",
         order = "original")
```


```{r}
# Distribution of numeric variables and correlation between them
pairs.panels(continuous_variables, density = TRUE, ellipses = TRUE, smooth = FALSE, lm = TRUE)
```

From these two correlation plots we can see that the numerical variables in the obese_complete dataset only weakly correlates with each other: most of them has indistinctive correlation in pairs, among which the strongest are 'Age ~ Bmi', 'FCVC ~ Bmi' and 'Age ~ TUE'.

# Model Training
The goal of best subset-selection is to find a small set of variables that most accurately predicts the dependent variable with *new* data. Thus the model should not include all variables, risking overfitting. Well, lets first build a model containing all variables and compare other models later on!

```{r, highlight=TRUE}
model_all <- lm(Bmi~., data = obese_train)
```

Building a model by trial and error takes a lot of time and working memory. It's best to build a model based on some algorithm. First let's prove this first statement. The following piece of code will calculate the number of possible models given the fact that we would like a model with at least 4 predicting variables.

```{r}
# load a necessary source
source("generate_formulas.R")
```

```{r, comment=NA}
# create a vector for all predicting variables except the dependent variable bmi

x_vars <- colnames(obese_complete)
x_vars <- x_vars[x_vars != "Bmi"]

# calculate the number of possible models

n_possible_models <- generate_formulas(p = 4, x_vars = x_vars, y_var = "Bmi") 
length(n_possible_models)
```

Given the assumption for a model containing 4 predicting variables, there are 1001 possible models. Thus, let's look at algorithms that calculate the "best model".

## Forward Stepwise Selection

For example we can use *forward selection*. This method start with no predictors and then iteratively adds the most contributive predictors until there's no significant improvement. Forward stepwise selection, is most useful for data sets that contain a lot of variables. We will use the Akiake's Information Criterion(AIC) in order to find a more parsimonious model since the AIC puts a penalty on adding more variables.


```{r, highlight=TRUE, comment=NA}
# start building a model by forward selection with AIC
model_forward <- stepAIC(model_all, direction = "forward", trace = FALSE)
summary(model_forward)

```
## Backward Stepwise Selection

Then there is also *backward selection* this method is the complete opposite of *forward selection*. Here the algorithm starts with the complete model and then iteratively starts eliminating the least significant variables until all significant variables remain. When dealing with a data set where there might be collinearity, this method is preferred over forward selection. Again, we will use the AIC to help us find a more parsimonious model.


```{r, highlight=TRUE, comment=NA}
model_backward <- stepAIC(model_all, direction = "backward", trace = FALSE)
summary(model_backward)
```

## LASSO Regression; step-by-step

The final method provided in this assignment is the LASSO regression method. The Least Absolute Shrinkage and Selection Operator is a powerful method since it can handle very large data sets. Furthermore this method avoids overfitting. The penalty as a result of tuning lambda, leads to the shrinking of the beta coefficients towards zero. The best value for lambda is chosen by k-fold cross-validation. The goal is to pick a value for lambda so the result is the lowest least squares. Enlarging lambda will result in smaller subsets of predicting variables.
```{r, warning=FALSE}

# specifying 10-fold cross-validation as training method
cross_train <- trainControl(method="cv", number = 10,
                            savePredictions = "all")


# create vector for potential lambda values
vector_lambda <- 10^seq(5, -5, length = 100)


# build LASSO model using training data and cross-validation
model_lasso <- train(Bmi ~ ., data = obese_train,
                     preProcess = c("center", "scale"),
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 1, lambda = vector_lambda),
                     trControl = cross_train)


# visualize how the log(lambda) affects the RMSE
plot(log(model_lasso$results$lambda), model_lasso$results$RMSE, 
     xlab = "log(lambda)",
     ylab = "RMSE",
     main = "RMSE value given lambda",
     xlim = c(-6,2))
```

```{r, comment=NA}
# find best lambda value
best_lambda <- model_lasso$bestTune$lambda
log(best_lambda)


# coefficients best model LASSO with lambda
coef(model_lasso$finalModel, best_lambda)
```

The LASSO regression method did only shrink the coefficients of *CALCAlways* to zero. Thus, should we build a model on this information, nearly every variable would be included into the best subset selection. However the goal of finding the best model is to find a model that most accurately predicts the outcome variable and also be parsimonious. 

Fortunately, with use of the variable importance function, we can see what variables show to have the greatest influence considering Bmi.

```{r}
# visualize the most important predictors
ggplot(varImp(model_lasso)) + labs(x = "Predictors", y = "Importance", title = "Predictors' influence in the LASSO model") + theme_classic()
```

For the sake of parsimony, we will select the 8most influential variables, which would lead to the following model:

$$Bmi = FamilyHistoryWithOverweightyes + CAECFrequently + Age + FCVC + MTRANSPublicTransportation + CALCSometimes + FAVC + FAF$$

## Dummy Coding

For our models we explicitly need some of the categories of certain categorical variables. For example, from the categorical variable *MTRANS* we only need Public_Transportation. Thus we will need some dummy variables.

For this we can use dummy_cols() function from the "fastDunmmies" package. After dummy-coding, we can finalize our models and start comparing which is the best!

```{r, include=FALSE}
obese_dummy_train <- obese_train
obese_dummy_val <- obese_val
obese_dummy_test <- obese_test

# creating dummies for the categorical variable MTRANS
obese_dummy_train <- dummy_cols(obese_dummy_train, select_columns = "MTRANS", remove_selected_columns = TRUE)
obese_dummy_val <- dummy_cols(obese_dummy_val, select_columns = "MTRANS", remove_selected_columns = TRUE)
obese_dummy_test <- dummy_cols(obese_dummy_test, select_columns = "MTRANS", remove_selected_columns = TRUE)

# deleting every dummy for MTRANS  except Public_Transport
obese_dummy_train <- subset(obese_dummy_train, select = -c(MTRANS_Walking, MTRANS_Automobile, MTRANS_Bike, MTRANS_Motorbike))
obese_dummy_val <- subset(obese_dummy_val, select = -c(MTRANS_Walking, MTRANS_Automobile, MTRANS_Bike, MTRANS_Motorbike))
obese_dummy_test <- subset(obese_dummy_test, select = -c(MTRANS_Walking, MTRANS_Automobile, MTRANS_Bike, MTRANS_Motorbike))

# creating dummies for the categorical variable CAEC
obese_dummy_train <- dummy_cols(obese_dummy_train, select_columns = "CAEC", remove_selected_columns = TRUE)
obese_dummy_val <- dummy_cols(obese_dummy_val, select_columns = "CAEC", remove_selected_columns = TRUE)
obese_dummy_test <- dummy_cols(obese_dummy_test, select_columns = "CAEC", remove_selected_columns = TRUE)

# deleting every dummy for CAEC except for CAEC_Frequently
obese_dummy_train <- subset(obese_dummy_train, select = -c(CAEC_no, CAEC_Sometimes, CAEC_Always))
obese_dummy_val <- subset(obese_dummy_val, select = -c(CAEC_no, CAEC_Sometimes, CAEC_Always))
obese_dummy_test <- subset(obese_dummy_test, select = -c(CAEC_no, CAEC_Sometimes, CAEC_Always))

# creating dummies for the categorical variable CALC
obese_dummy_train <- dummy_cols(obese_dummy_train, select_columns = "CALC", remove_selected_columns = TRUE)
obese_dummy_val <- dummy_cols(obese_dummy_val, select_columns = "CALC", remove_selected_columns = TRUE)
obese_dummy_test <- dummy_cols(obese_dummy_test, select_columns = "CALC", remove_selected_columns = TRUE)

# deleting every dummy for CALC except for CALC_Sometimes
obese_dummy_train <- subset(obese_dummy_train, select = -c(CALC_Frequently, CALC_no, CALC_Always))
obese_dummy_val <- subset(obese_dummy_val, select = -c(CALC_Frequently, CALC_no, CALC_Always))
obese_dummy_test <- subset(obese_dummy_test, select = -c(CALC_Frequently, CALC_no, CALC_Always))
```
## Model Comparison

At this point we have 4 possible best models. <br>
The **first** model is contains all variables (14 variables)<br>
The **second** model comes from forward stepwise selection (10 variables) <br>
The **third** model comes from backward stepwise selection (10 variables) <br>
Finally, the **fourth** model comes from LASSO regression (8 variables)

Considering the fact that the models regarding forward and backward selection are identical, we proceed to compare just **3** models.

```{r}

model_all <- lm(Bmi~., data=obese_dummy_train)

model_forward <- lm(Bmi ~ Age + family_history_with_overweight + 
                      FAVC + FCVC + CAEC_Frequently + FAF + 
                      CALC_Sometimes + MTRANS_Public_Transportation +
                      SCC + CH2O, data = obese_dummy_train)

model_lasso <- lm(Bmi ~ Age + family_history_with_overweight + 
                      FAVC + FCVC + CAEC_Frequently + FAF + 
                      CALC_Sometimes + MTRANS_Public_Transportation, 
                      data = obese_dummy_train)

```
Let's now compare the models with respect to the train data. <br>
- What models has the lowest RSS ? <br>
- What model has the lowest AIC ? <br>
- What model has the highest R-squared ?
```{r, comment=NA}
# analysis of variance
anova(model_all, model_forward, model_lasso)
```

```{r}
# AIC criteria
AIC(model_all)
AIC(model_forward)
AIC(model_lasso)
```

```{r}
# Explained variance (r-squared)
summary(model_all)$r.squared
summary(model_forward)$r.squared
summary(model_lasso)$r.squared
```

Let's now compare the models with respect to the test data. <br>
- Which model does the best in predicting Bmi with new data?

```{r}
# make the predictions for all models
predictions_model_all <- predict(model_all, newdata = obese_dummy_test)
predictions_model_forward <- predict(model_forward, newdata = obese_dummy_test)
predictions_model_lasso <- predict(model_lasso, newdata = obese_dummy_test)


# calculating RMSE's
mse <- function(y_true, y_pred) {
  mean((y_true - y_pred)^2)
}

mse_pred <- c(
  mse(obese_dummy_test$Bmi, predictions_model_all),
  mse(obese_dummy_test$Bmi, predictions_model_forward),
  mse(obese_dummy_test$Bmi, predictions_model_lasso))

# calculating R-squared
rsquareds <- c(
  R2(predictions_model_all,obese_dummy_test$Bmi),
  R2(predictions_model_forward,obese_dummy_test$Bmi),
  R2(predictions_model_lasso,obese_dummy_test$Bmi))

# plotting RMSE - comparison
tibble(Method = as_factor(c("All", "Forward", "LASSO")), MSE = mse_pred) %>% 
  ggplot(aes(x = Method, y = MSE, fill = Method)) +
  geom_bar(stat = "identity", col = "black") + 
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Comparison of test set MSE for different prediction methods") +
  scale_fill_viridis_d() # different colour scale

# plotting R-squared - comparison
tibble(Method = as_factor(c("All", "Forward", "LASSO")), Rsquared = rsquareds) %>%
  ggplot(aes(x = Method, y = Rsquared, fill = Method)) +
  geom_bar(stat = "identity", col = "black") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Comparison of test set Rsquared for different prediction methods") +
  scale_fill_viridis_d() # different colour scale
  

```

**the model with all variables comes out best in every comparison**

# Model Evaluation & VIF


# Checking Assumptions

To be able to create a linear model, several assumptions have to met. We will now check for outliers, normality.

The diagRegressionPlots() function makes a few plots automatically which are useful in determining whether linear regression is working on a data set. This is part of the "HannayIntroStats" package (https://github.com/khannay/HannayIntroStats).

```{r}
diagRegressionPlots <- function(regression.obj, cex=1)
{
  par(mfrow=c(2,2))
  #Grab the names of the two variables
  var.names=attr(regression.obj$model, "names")
  
  x=regression.obj$model[,2]
  y=regression.obj$model[,1]
  
  
  qqnorm(as.vector(regression.obj$residuals), main="Normality check QQ Plot", cex=cex)
  qqline(as.vector(regression.obj$residuals), col='red')
  plot(x,regression.obj$residuals, main='Residual Plot', ylab='Residuals', xlab=var.names[2], cex=cex)
  hist(regression.obj$residuals, main='Histogram for the Residuals', xlab='Residual Value', col='lightblue')
  
  #plot the regression
  title=paste(c('Model Fit R^2=', round(summary(regression.obj)$r.squared,2)), collapse=" ")
  plot(x,y, main=title, xlab=var.names[2], ylab=var.names[1], cex=cex)
  abline(regression.obj, col='red')
  par(mfrow=c(1,1))
}
```

```{r}
# This call of diagRegressionPlots() should produce four plots for the assumptions checking of a linear model
diagRegressionPlots(model_forward)
```

## linearity

```{r}
plot(model_forward, 1)
```

interpretation: our model is curved but it doesn't diverge from a horizontal line very much -> still linear

## Checking whether the predictor matrix is full rank

```{r}
dim(obese_dummy_train)
```

The dataset contains 1479 rows and 15 columns. Hence, the assumption of the predictor matrix to be full rank is satisfied. Moreover, the predictors are not linear combinations of other predictors.

## Heteroscedasticity

```{r}
plot(model_forward, 3)
```
no obvious trend, distributed on both sides of the red line, red line is quite flat

## Normality

```{r}
plot(model_forward, 2)

ggplot(data = obese_train, aes(x = model_forward$residuals)) +
  geom_histogram(aes(y= ..density..), col = 'black', fill = 'steelblue', bins = 60) +
  stat_function(fun = dnorm, 
                args = list(mean = mean(model_forward$residuals), 
                                        sd = sd(model_forward$residuals)),
                col = "red",
                size = 1)

mean(model_forward$residuals)
median(model_forward$residuals)
```


Slight left skew , but still quite normally distributed

The distribution shape of our data mostly overlaps with the red reference line, which represents for perfect normal distribution

## Checking Influential observations and outliers

```{r}
plot(rstudent(model_forward))
plot(hatvalues(model_forward))
(maxDs2 <- sort(hatvalues(model_forward)) %>% names() %>% tail(10) %>% as.numeric())
```

no obvious outliers

```{r}
cd <- cooks.distance(model_forward)
plot(cd)
(maxDs <- sort(cd) %>% names() %>% tail(1) %>% as.numeric())
```

# Visualizations

# Answering Research Question

# Discussion & Limitations


